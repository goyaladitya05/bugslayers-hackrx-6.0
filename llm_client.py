from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables import Runnable
from langchain_core.prompts import PromptTemplate
from typing import List
from langchain_core.documents import Document
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Set up Gemini LLM
llm = ChatGoogleGenerativeAI(
    model="models/gemini-2.5-pro",
    google_api_key=os.getenv("GOOGLE_API_KEY"),
    temperature=0.2
)

def build_prompt(query: str, docs: List[Document]) -> str:
    """
    Constructs a prompt from user query and relevant documents.

    Args:
        query (str): User question.
        docs (List[Document]): Retrieved context documents.

    Returns:
        str: Prompt to send to LLM.
    """
    context = "\n\n".join([doc.page_content for doc in docs])
    prompt = f"""You are a legal assistant. Based on the following policy/contract clauses, answer the user query clearly and factually.

Context:
{context}

Question: {query}

Answer:"""
    return prompt


def get_llm_response(query: str, docs: List[Document]) -> str:
    """
    Calls the Gemini model with the generated prompt.

    Args:
        query (str): The user's question.
        docs (List[Document]): Contextual documents for retrieval-augmented generation.

    Returns:
        str: Answer generated by the LLM.
    """
    prompt = build_prompt(query, docs)
    response = llm.invoke(prompt)
    return response.content.strip()

from langchain_google_genai import GoogleGenerativeAIEmbeddings

# Instantiate embedding model
embedding_model = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=os.getenv("GOOGLE_API_KEY")
)
